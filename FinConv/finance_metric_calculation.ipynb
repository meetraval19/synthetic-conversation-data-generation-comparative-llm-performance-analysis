{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb556a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ddc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/Users/meetraval/Desktop/data/finance/Finance_Final_Data.csv\"\n",
    "\n",
    "model_output_path = \"/Users/meetraval/Desktop/data/finance/processed/deepseek\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f60ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea007c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the .txt files\n",
    "\n",
    "# Create a list to store the contents in order of df index\n",
    "new_column_data = []\n",
    "\n",
    "for idx in df.index:\n",
    "    file_path = os.path.join(model_output_path, f\"{idx}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content_lst = f.read().strip().split(\"```json\")\n",
    "            if len(content_lst) > 1:\n",
    "              content = json.loads(content_lst[1].split(\"```\")[0].replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "              try:\n",
    "                content = json.loads(content_lst[0])\n",
    "              except Exception as e:\n",
    "                content_1 = \"\"\"{\\n  \"topics\":\"\"\" + content_lst[0].split(\"\"\"{\\n  \"topics\":\"\"\")[1]\n",
    "                content = json.loads(content_1.replace(\"\\n\", \"\"))\n",
    "            new_column_data.append(content)\n",
    "    else:\n",
    "        new_column_data.append(None)  # or '' or np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c1a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n",
      "2011\n"
     ]
    }
   ],
   "source": [
    "print(len(new_column_data))\n",
    "print(len(df['Conversation'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c9fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as a new column\n",
    "df['predicted_response'] = new_column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868ff863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Clean json responses\n",
    "topics_pred, topics_true, sents_pred, sents_true = [], [], [], []\n",
    "for ii in range(len(df)):\n",
    "    json_true = ast.literal_eval(df['Filename_Json'][ii])\n",
    "    json_pred = df['predicted_response'][ii]\n",
    "    topic_true = list(json_true.keys())\n",
    "    sent_true = list(json_true.values())\n",
    "    topic_pred = [x['topic'] for x in json_pred['topics']]\n",
    "    sent_pred = [x['sentiment'] for x in json_pred['topics']]\n",
    "    topics_pred.append(topic_pred)\n",
    "    topics_true.append(topic_true)\n",
    "    sents_pred.append(sent_pred)\n",
    "    sents_true.append(sent_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24802770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'Asset Allocation Strategy':\n",
      "[[1719  130]\n",
      " [  12  150]]\n",
      "\n",
      "Label 'Cash Flow Management':\n",
      "[[1669   90]\n",
      " [   0  252]]\n",
      "\n",
      "Label 'Creditworthiness Evaluation':\n",
      "[[1711    4]\n",
      " [   2  294]]\n",
      "\n",
      "Label 'Debt-to-Equity Ratio Assessment':\n",
      "[[1784    2]\n",
      " [   2  223]]\n",
      "\n",
      "Label 'Financial Statement Analysis':\n",
      "[[1777   15]\n",
      " [   7  212]]\n",
      "\n",
      "Label 'Forecasting and Projections':\n",
      "[[1725   44]\n",
      " [  22  220]]\n",
      "\n",
      "Label 'Interest Rate Impact Analysis':\n",
      "[[1773   12]\n",
      " [   7  219]]\n",
      "\n",
      "Label 'Internal Controls and Governance':\n",
      "[[1713    7]\n",
      " [   1  290]]\n",
      "\n",
      "Label 'Investment Performance Review':\n",
      "[[1722  104]\n",
      " [   0  185]]\n",
      "\n",
      "Label 'Legal and Contractual Obligations':\n",
      "[[1678    3]\n",
      " [   8  322]]\n",
      "\n",
      "Label 'Liquidity Risk Evaluation':\n",
      "[[1732   30]\n",
      " [  12  237]]\n",
      "\n",
      "Label 'Market Conditions Impact':\n",
      "[[1405  400]\n",
      " [   2  204]]\n",
      "\n",
      "Label 'Negotiation of Financial Terms':\n",
      "[[1689   22]\n",
      " [   2  298]]\n",
      "\n",
      "Label 'Portfolio Diversification Review':\n",
      "[[1644  208]\n",
      " [   5  154]]\n",
      "\n",
      "Label 'Regulatory Compliance Check':\n",
      "[[1655   52]\n",
      " [   0  304]]\n",
      "\n",
      "Label 'Risk Tolerance Assessment':\n",
      "[[1732   54]\n",
      " [   4  221]]\n",
      "\n",
      "Label 'Scenario and Sensitivity Analysis':\n",
      "[[1751   13]\n",
      " [   9  238]]\n",
      "\n",
      "Label 'Strategic Financial Planning':\n",
      "[[1535  272]\n",
      " [   5  199]]\n",
      "\n",
      "Label 'Tax Implication Review':\n",
      "[[1691    1]\n",
      " [   3  316]]\n",
      "\n",
      "Label 'Valuation Metrics Discussion':\n",
      "[[1740    0]\n",
      " [   5  266]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meetraval/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:875: UserWarning: unknown class(es) ['Debt-to-Eity Ratio Assessment', 'Negot Financial Terms', 'bt-to-Equity Ratio Assessment'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Step 1: Binarize\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(topics_true)\n",
    "y_pred_bin = mlb.transform(topics_pred)\n",
    "\n",
    "# Step 2: Compute confusion matrices\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Step 3: Print with labels\n",
    "for label, matrix in zip(mlb.classes_, conf_matrices):\n",
    "    print(f\"Label '{label}':\\n{matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "346ff038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "        Asset Allocation Strategy       0.54      0.93      0.68       162\n",
      "             Cash Flow Management       0.74      1.00      0.85       252\n",
      "      Creditworthiness Evaluation       0.99      0.99      0.99       296\n",
      "  Debt-to-Equity Ratio Assessment       0.99      0.99      0.99       225\n",
      "     Financial Statement Analysis       0.93      0.97      0.95       219\n",
      "      Forecasting and Projections       0.83      0.91      0.87       242\n",
      "    Interest Rate Impact Analysis       0.95      0.97      0.96       226\n",
      " Internal Controls and Governance       0.98      1.00      0.99       291\n",
      "    Investment Performance Review       0.64      1.00      0.78       185\n",
      "Legal and Contractual Obligations       0.99      0.98      0.98       330\n",
      "        Liquidity Risk Evaluation       0.89      0.95      0.92       249\n",
      "         Market Conditions Impact       0.34      0.99      0.50       206\n",
      "   Negotiation of Financial Terms       0.93      0.99      0.96       300\n",
      " Portfolio Diversification Review       0.43      0.97      0.59       159\n",
      "      Regulatory Compliance Check       0.85      1.00      0.92       304\n",
      "        Risk Tolerance Assessment       0.80      0.98      0.88       225\n",
      "Scenario and Sensitivity Analysis       0.95      0.96      0.96       247\n",
      "     Strategic Financial Planning       0.42      0.98      0.59       204\n",
      "           Tax Implication Review       1.00      0.99      0.99       319\n",
      "     Valuation Metrics Discussion       1.00      0.98      0.99       271\n",
      "\n",
      "                        micro avg       0.77      0.98      0.86      4912\n",
      "                        macro avg       0.81      0.98      0.87      4912\n",
      "                     weighted avg       0.84      0.98      0.89      4912\n",
      "                      samples avg       0.81      0.98      0.88      4912\n",
      "\n",
      "Subset Accuracy: 0.5117\n",
      "Micro Precision: 0.7665549704802936\n",
      "Micro Recall: 0.9780130293159609\n",
      "Micro F1 Score: 0.8594686465694605\n",
      "Macro Precision: 0.809003464821313\n",
      "Macro Recall: 0.9763080649494892\n",
      "Macro F1 Score: 0.8673457992804353\n",
      "Weighted Precision: 0.8397583112382818\n",
      "Weighted Recall: 0.9780130293159609\n",
      "Weighted F1 Score: 0.8889653245208301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "# --- 1. Per-Class & Aggregate Report\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_))\n",
    "\n",
    "# --- 2. Subset Accuracy (exact match)\n",
    "subset_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "\n",
    "# --- 3. Micro-Averaged Scores\n",
    "print(\"Micro Precision:\", precision_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro Recall:\", recall_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "\n",
    "# --- 4. Macro-Averaged Scores\n",
    "print(\"Macro Precision:\", precision_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "\n",
    "# --- 5. Weighted-Averaged Scores\n",
    "print(\"Weighted Precision:\", precision_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted Recall:\", recall_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f883da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'negative':\n",
      "[[ 471    3]\n",
      " [  92 1445]]\n",
      "\n",
      "Label 'neutral':\n",
      "[[490 134]\n",
      " [434 953]]\n",
      "\n",
      "Label 'positive':\n",
      "[[ 177  298]\n",
      " [   0 1536]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meetraval/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:875: UserWarning: unknown class(es) [''] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Binarize\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(sents_true)\n",
    "y_pred_bin = mlb.transform(sents_pred)\n",
    "\n",
    "# Step 2: Compute confusion matrices\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Step 3: Print with labels\n",
    "for label, matrix in zip(mlb.classes_, conf_matrices):\n",
    "    print(f\"Label '{label}':\\n{matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a7e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.94      0.97      1537\n",
      "     neutral       0.88      0.69      0.77      1387\n",
      "    positive       0.84      1.00      0.91      1536\n",
      "\n",
      "   micro avg       0.90      0.88      0.89      4460\n",
      "   macro avg       0.90      0.88      0.88      4460\n",
      "weighted avg       0.90      0.88      0.89      4460\n",
      " samples avg       0.91      0.88      0.88      4460\n",
      "\n",
      "Subset Accuracy: 0.6280\n",
      "Micro Precision: 0.9004348821240559\n",
      "Micro Recall: 0.8820627802690583\n",
      "Micro F1 Score: 0.891154151092989\n",
      "Macro Precision: 0.9040555797350337\n",
      "Macro Recall: 0.8757458614763575\n",
      "Macro F1 Score: 0.8833863974816308\n",
      "Weighted Precision: 0.9049896916763274\n",
      "Weighted Recall: 0.8820627802690583\n",
      "Weighted F1 Score: 0.8871796553629027\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Per-Class & Aggregate Report\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_))\n",
    "\n",
    "# --- 2. Subset Accuracy (exact match)\n",
    "subset_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "\n",
    "# --- 3. Micro-Averaged Scores\n",
    "print(\"Micro Precision:\", precision_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro Recall:\", recall_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "\n",
    "# --- 4. Macro-Averaged Scores\n",
    "print(\"Macro Precision:\", precision_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "\n",
    "# --- 5. Weighted-Averaged Scores\n",
    "print(\"Weighted Precision:\", precision_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted Recall:\", recall_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f4ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
