{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7e5b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e4b099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/Users/meetraval/Desktop/data/healthcare/Healthcare_Final_Data.csv\"\n",
    "\n",
    "model_output_path = \"/Users/meetraval/Desktop/data/healthcare/processed/deepseek\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "681ef621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file_path)\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e730e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the .txt files\n",
    "\n",
    "# Create a list to store the contents in order of df index\n",
    "new_column_data = []\n",
    "\n",
    "for idx in df.index:\n",
    "    file_path = os.path.join(model_output_path, f\"{idx}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content_lst = f.read().strip().split(\"```json\")\n",
    "            if len(content_lst) > 1:\n",
    "              content = json.loads(content_lst[1].split(\"```\")[0].replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "              try:\n",
    "                content = json.loads(content_lst[0])\n",
    "              except Exception as e:\n",
    "                content_1 = \"\"\"{\\n  \"topics\":\"\"\" + content_lst[0].split(\"\"\"{\\n  \"topics\":\"\"\")[1]\n",
    "                content = json.loads(content_1.replace(\"\\n\", \"\"))\n",
    "            new_column_data.append(content)\n",
    "    else:\n",
    "        new_column_data.append(None)  # or '' or np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffe8de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2288\n",
      "2288\n"
     ]
    }
   ],
   "source": [
    "print(len(new_column_data))\n",
    "print(len(df['Conversation'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143c1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4161ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as a new column\n",
    "df['predicted_response'] = new_column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e85174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for ii in range(len(df)):\n",
    "#     pred = df['predicted_response'][ii]\n",
    "#     if pred is None:\n",
    "#         print(ii)\n",
    "# print(df['predicted_response'][0]['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47b91aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Clean json responses\n",
    "topics_pred, topics_true, sents_pred, sents_true = [], [], [], []\n",
    "for ii in range(2082):\n",
    "    json_true = ast.literal_eval(df['Filename_Json'][ii])\n",
    "    json_pred = df['predicted_response'][ii]\n",
    "    topic_true = list(json_true.keys())\n",
    "    sent_true = list(json_true.values())\n",
    "    topic_pred = [x['topic'] for x in json_pred['topics']]\n",
    "    sent_pred = [x['sentiment'] for x in json_pred['topics']]\n",
    "    topics_pred.append(topic_pred)\n",
    "    topics_true.append(topic_true)\n",
    "    sents_pred.append(sent_pred)\n",
    "    sents_true.append(sent_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f184213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'Care Barriers':\n",
      "[[1614  225]\n",
      " [ 212   31]]\n",
      "\n",
      "Label 'Care Plan Update':\n",
      "[[1606  265]\n",
      " [ 176   35]]\n",
      "\n",
      "Label 'Diagnosis Communication':\n",
      "[[1626  188]\n",
      " [ 254   14]]\n",
      "\n",
      "Label 'Discharge Planning':\n",
      "[[1368  387]\n",
      " [ 253   74]]\n",
      "\n",
      "Label 'Documentation Review':\n",
      "[[1568  242]\n",
      " [ 232   40]]\n",
      "\n",
      "Label 'Emergency Concerns':\n",
      "[[1657  243]\n",
      " [ 162   20]]\n",
      "\n",
      "Label 'Follow Up':\n",
      "[[1467  349]\n",
      " [ 177   89]]\n",
      "\n",
      "Label 'Medical History':\n",
      "[[1639  159]\n",
      " [ 168  116]]\n",
      "\n",
      "Label 'Medication Review':\n",
      "[[1502  295]\n",
      " [ 184  101]]\n",
      "\n",
      "Label 'Patient Education':\n",
      "[[1520  322]\n",
      " [ 205   35]]\n",
      "\n",
      "Label 'Referral Coordination':\n",
      "[[1605  222]\n",
      " [ 200   55]]\n",
      "\n",
      "Label 'Risk Assessment':\n",
      "[[1550  246]\n",
      " [ 241   45]]\n",
      "\n",
      "Label 'Shared Decision':\n",
      "[[1556  257]\n",
      " [ 234   35]]\n",
      "\n",
      "Label 'Side Effects':\n",
      "[[1564  274]\n",
      " [ 211   33]]\n",
      "\n",
      "Label 'Support Services':\n",
      "[[1561  363]\n",
      " [ 124   34]]\n",
      "\n",
      "Label 'Symptom Assessment':\n",
      "[[1628  198]\n",
      " [ 151  105]]\n",
      "\n",
      "Label 'Team Communication':\n",
      "[[1455  383]\n",
      " [ 178   66]]\n",
      "\n",
      "Label 'Test Results':\n",
      "[[1522  278]\n",
      " [ 154  128]]\n",
      "\n",
      "Label 'Therapy Adherence':\n",
      "[[1561  254]\n",
      " [ 221   46]]\n",
      "\n",
      "Label 'Treatment Plan':\n",
      "[[1376  476]\n",
      " [ 129  101]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meetraval/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:875: UserWarning: unknown class(es) ['Telemedicine'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Step 1: Binarize\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(topics_true)\n",
    "y_pred_bin = mlb.transform(topics_pred)\n",
    "\n",
    "# Step 2: Compute confusion matrices\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Step 3: Print with labels\n",
    "for label, matrix in zip(mlb.classes_, conf_matrices):\n",
    "    print(f\"Label '{label}':\\n{matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "948c0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "          Care Barriers       0.12      0.13      0.12       243\n",
      "       Care Plan Update       0.12      0.17      0.14       211\n",
      "Diagnosis Communication       0.07      0.05      0.06       268\n",
      "     Discharge Planning       0.16      0.23      0.19       327\n",
      "   Documentation Review       0.14      0.15      0.14       272\n",
      "     Emergency Concerns       0.08      0.11      0.09       182\n",
      "              Follow Up       0.20      0.33      0.25       266\n",
      "        Medical History       0.42      0.41      0.42       284\n",
      "      Medication Review       0.26      0.35      0.30       285\n",
      "      Patient Education       0.10      0.15      0.12       240\n",
      "  Referral Coordination       0.20      0.22      0.21       255\n",
      "        Risk Assessment       0.15      0.16      0.16       286\n",
      "        Shared Decision       0.12      0.13      0.12       269\n",
      "           Side Effects       0.11      0.14      0.12       244\n",
      "       Support Services       0.09      0.22      0.12       158\n",
      "     Symptom Assessment       0.35      0.41      0.38       256\n",
      "     Team Communication       0.15      0.27      0.19       244\n",
      "           Test Results       0.32      0.45      0.37       282\n",
      "      Therapy Adherence       0.15      0.17      0.16       267\n",
      "         Treatment Plan       0.18      0.44      0.25       230\n",
      "\n",
      "              micro avg       0.18      0.24      0.20      5069\n",
      "              macro avg       0.17      0.23      0.20      5069\n",
      "           weighted avg       0.18      0.24      0.20      5069\n",
      "            samples avg       0.18      0.24      0.20      5069\n",
      "\n",
      "Subset Accuracy: 0.0043\n",
      "Micro Precision: 0.17616049201932932\n",
      "Micro Recall: 0.23732491615703294\n",
      "Micro F1 Score: 0.20221886031265757\n",
      "Macro Precision: 0.17334748459118116\n",
      "Macro Recall: 0.23358663842383756\n",
      "Macro F1 Score: 0.1952648582779614\n",
      "Weighted Precision: 0.17935313459949637\n",
      "Weighted Recall: 0.23732491615703294\n",
      "Weighted F1 Score: 0.2008040902809228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "# --- 1. Per-Class & Aggregate Report\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_))\n",
    "\n",
    "# --- 2. Subset Accuracy (exact match)\n",
    "subset_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "\n",
    "# --- 3. Micro-Averaged Scores\n",
    "print(\"Micro Precision:\", precision_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro Recall:\", recall_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "\n",
    "# --- 4. Macro-Averaged Scores\n",
    "print(\"Macro Precision:\", precision_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "\n",
    "# --- 5. Weighted-Averaged Scores\n",
    "print(\"Weighted Precision:\", precision_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted Recall:\", recall_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68853c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'negative':\n",
      "[[ 176  333]\n",
      " [ 503 1070]]\n",
      "\n",
      "Label 'neutral':\n",
      "[[297 328]\n",
      " [733 724]]\n",
      "\n",
      "Label 'positive':\n",
      "[[  38  476]\n",
      " [  98 1470]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Binarize\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(sents_true)\n",
    "y_pred_bin = mlb.transform(sents_pred)\n",
    "\n",
    "# Step 2: Compute confusion matrices\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Step 3: Print with labels\n",
    "for label, matrix in zip(mlb.classes_, conf_matrices):\n",
    "    print(f\"Label '{label}':\\n{matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8529bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.68      0.72      1573\n",
      "     neutral       0.69      0.50      0.58      1457\n",
      "    positive       0.76      0.94      0.84      1568\n",
      "\n",
      "   micro avg       0.74      0.71      0.73      4598\n",
      "   macro avg       0.74      0.70      0.71      4598\n",
      "weighted avg       0.74      0.71      0.71      4598\n",
      " samples avg       0.74      0.71      0.71      4598\n",
      "\n",
      "Subset Accuracy: 0.2397\n",
      "Micro Precision: 0.7416496250852079\n",
      "Micro Recall: 0.709873858199217\n",
      "Micro F1 Score: 0.7254139348816536\n",
      "Macro Precision: 0.73542002412152\n",
      "Macro Recall: 0.7048801079850247\n",
      "Macro F1 Score: 0.7109539224884464\n",
      "Weighted Precision: 0.7365892596329741\n",
      "Weighted Recall: 0.709873858199217\n",
      "Weighted F1 Score: 0.714193584077433\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Per-Class & Aggregate Report\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_))\n",
    "\n",
    "# --- 2. Subset Accuracy (exact match)\n",
    "subset_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "\n",
    "# --- 3. Micro-Averaged Scores\n",
    "print(\"Micro Precision:\", precision_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro Recall:\", recall_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "\n",
    "# --- 4. Macro-Averaged Scores\n",
    "print(\"Macro Precision:\", precision_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "\n",
    "# --- 5. Weighted-Averaged Scores\n",
    "print(\"Weighted Precision:\", precision_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted Recall:\", recall_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30fbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec12f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
