{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e5b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e4b099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/Users/meetraval/Desktop/data/Legal/Legal_Final_Data.csv\"\n",
    "\n",
    "model_output_path = \"/Users/meetraval/Desktop/data/Legal/deepseek\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681ef621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e730e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the .txt files\n",
    "\n",
    "# Create a list to store the contents in order of df index\n",
    "new_column_data = []\n",
    "\n",
    "for idx in df.index:\n",
    "    file_path = os.path.join(model_output_path, f\"{idx}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content_lst = f.read().strip().split(\"```json\")\n",
    "            if len(content_lst) > 1:\n",
    "              content = json.loads(content_lst[1].split(\"```\")[0].replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "              try:\n",
    "                content = json.loads(content_lst[0])\n",
    "              except Exception as e:\n",
    "                content_1 = \"\"\"{\\n  \"topics\":\"\"\" + content_lst[0].split(\"\"\"{\\n  \"topics\":\"\"\")[1]\n",
    "                content = json.loads(content_1.replace(\"\\n\", \"\"))\n",
    "            new_column_data.append(content)\n",
    "    else:\n",
    "        print(idx)\n",
    "        new_column_data.append(None)  # or '' or np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe8de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2166\n",
      "2166\n"
     ]
    }
   ],
   "source": [
    "print(len(new_column_data))\n",
    "print(len(df['Conversation'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4161ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as a new column\n",
    "df['predicted_response'] = new_column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b91aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Clean json responses\n",
    "topics_pred, topics_true, sents_pred, sents_true = [], [], [], []\n",
    "for ii in range(len(df)):\n",
    "    json_true = ast.literal_eval(df['Filename_Json'][ii])\n",
    "    json_pred = df['predicted_response'][ii]\n",
    "    topic_true = list(json_true.keys())\n",
    "    sent_true = list(json_true.values())\n",
    "    topic_pred = [x['topic'] for x in json_pred['topics']]\n",
    "    sent_pred = [x['sentiment'] for x in json_pred['topics']]\n",
    "    topics_pred.append(topic_pred)\n",
    "    topics_true.append(topic_true)\n",
    "    sents_pred.append(sent_pred)\n",
    "    sents_true.append(sent_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f184213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'Applicable Laws and Regulations':\n",
      "[[1888   47]\n",
      " [  39  192]]\n",
      "\n",
      "Label 'Client Objectives and Priorities':\n",
      "[[1850   29]\n",
      " [   5  282]]\n",
      "\n",
      "Label 'Communication with Opposing Parties':\n",
      "[[1908   10]\n",
      " [   5  243]]\n",
      "\n",
      "Label 'Confidentiality and Privilege':\n",
      "[[1835    0]\n",
      " [   1  330]]\n",
      "\n",
      "Label 'Contract Terms and Obligations':\n",
      "[[1861   26]\n",
      " [  12  267]]\n",
      "\n",
      "Label 'Costs, Fees, and Budgeting':\n",
      "[[1795   16]\n",
      " [   4  351]]\n",
      "\n",
      "Label 'Dispute Resolution Options':\n",
      "[[1852   40]\n",
      " [  11  263]]\n",
      "\n",
      "Label 'Drafting and Reviewing Legal Documents':\n",
      "[[1890   44]\n",
      " [   5  227]]\n",
      "\n",
      "Label 'Enforcement and Implementation Issues':\n",
      "[[1932    4]\n",
      " [  16  214]]\n",
      "\n",
      "Label 'Evidence and Documentation Review':\n",
      "[[1866   35]\n",
      " [   9  256]]\n",
      "\n",
      "Label 'Governing Law and Jurisdiction':\n",
      "[[1885    1]\n",
      " [  20  260]]\n",
      "\n",
      "Label 'Legal Argument and Case Theory':\n",
      "[[1900   14]\n",
      " [  15  237]]\n",
      "\n",
      "Label 'Legal Issue Identification':\n",
      "[[1873   62]\n",
      " [  33  198]]\n",
      "\n",
      "Label 'Regulatory Compliance Check':\n",
      "[[1884  108]\n",
      " [   0  174]]\n",
      "\n",
      "Label 'Risk Assessment and Mitigation':\n",
      "[[1704  171]\n",
      " [   9  282]]\n",
      "\n",
      "Label 'Settlement Opportunities and Strategy':\n",
      "[[1874   58]\n",
      " [  24  210]]\n",
      "\n",
      "Label 'Strategy and Action Plan Development':\n",
      "[[1310  602]\n",
      " [   5  249]]\n",
      "\n",
      "Label 'Timeline and Key Deadlines':\n",
      "[[1759   84]\n",
      " [   2  321]]\n",
      "\n",
      "Label 'Updates and Next Steps Planning':\n",
      "[[1723  164]\n",
      " [ 153  126]]\n",
      "\n",
      "Label 'Witness Identification and Preparation':\n",
      "[[1892    8]\n",
      " [   0  266]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Step 1: Binarize\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(topics_true)\n",
    "y_pred_bin = mlb.transform(topics_pred)\n",
    "\n",
    "# Step 2: Compute confusion matrices\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Step 3: Print with labels\n",
    "for label, matrix in zip(mlb.classes_, conf_matrices):\n",
    "    print(f\"Label '{label}':\\n{matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "948c0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "       Applicable Laws and Regulations       0.80      0.83      0.82       231\n",
      "      Client Objectives and Priorities       0.91      0.98      0.94       287\n",
      "   Communication with Opposing Parties       0.96      0.98      0.97       248\n",
      "         Confidentiality and Privilege       1.00      1.00      1.00       331\n",
      "        Contract Terms and Obligations       0.91      0.96      0.93       279\n",
      "            Costs, Fees, and Budgeting       0.96      0.99      0.97       355\n",
      "            Dispute Resolution Options       0.87      0.96      0.91       274\n",
      "Drafting and Reviewing Legal Documents       0.84      0.98      0.90       232\n",
      " Enforcement and Implementation Issues       0.98      0.93      0.96       230\n",
      "     Evidence and Documentation Review       0.88      0.97      0.92       265\n",
      "        Governing Law and Jurisdiction       1.00      0.93      0.96       280\n",
      "        Legal Argument and Case Theory       0.94      0.94      0.94       252\n",
      "            Legal Issue Identification       0.76      0.86      0.81       231\n",
      "           Regulatory Compliance Check       0.62      1.00      0.76       174\n",
      "        Risk Assessment and Mitigation       0.62      0.97      0.76       291\n",
      " Settlement Opportunities and Strategy       0.78      0.90      0.84       234\n",
      "  Strategy and Action Plan Development       0.29      0.98      0.45       254\n",
      "            Timeline and Key Deadlines       0.79      0.99      0.88       323\n",
      "       Updates and Next Steps Planning       0.43      0.45      0.44       279\n",
      "Witness Identification and Preparation       0.97      1.00      0.99       266\n",
      "\n",
      "                             micro avg       0.76      0.93      0.84      5316\n",
      "                             macro avg       0.82      0.93      0.86      5316\n",
      "                          weighted avg       0.82      0.93      0.86      5316\n",
      "                           samples avg       0.80      0.93      0.85      5316\n",
      "\n",
      "Subset Accuracy: 0.4769\n",
      "Micro Precision: 0.764642250038634\n",
      "Micro Recall: 0.9307750188111362\n",
      "Micro F1 Score: 0.8395690167133283\n",
      "Macro Precision: 0.8160383497668272\n",
      "Macro Recall: 0.9294747846292545\n",
      "Macro F1 Score: 0.857676538530554\n",
      "Weighted Precision: 0.8228521560925399\n",
      "Weighted Recall: 0.9307750188111362\n",
      "Weighted F1 Score: 0.8626922378985681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "# --- 1. Per-Class & Aggregate Report\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_))\n",
    "\n",
    "# --- 2. Subset Accuracy (exact match)\n",
    "subset_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "\n",
    "# --- 3. Micro-Averaged Scores\n",
    "print(\"Micro Precision:\", precision_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro Recall:\", recall_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "\n",
    "# --- 4. Macro-Averaged Scores\n",
    "print(\"Macro Precision:\", precision_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "\n",
    "# --- 5. Weighted-Averaged Scores\n",
    "print(\"Weighted Precision:\", precision_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted Recall:\", recall_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68853c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'negative':\n",
      "[[ 535    3]\n",
      " [ 215 1413]]\n",
      "\n",
      "Label 'neutral':\n",
      "[[ 495  154]\n",
      " [ 484 1033]]\n",
      "\n",
      "Label 'positive':\n",
      "[[ 222  281]\n",
      " [   3 1660]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Binarize\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(sents_true)\n",
    "y_pred_bin = mlb.transform(sents_pred)\n",
    "\n",
    "# Step 2: Compute confusion matrices\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Step 3: Print with labels\n",
    "for label, matrix in zip(mlb.classes_, conf_matrices):\n",
    "    print(f\"Label '{label}':\\n{matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8529bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.87      0.93      1628\n",
      "     neutral       0.87      0.68      0.76      1517\n",
      "    positive       0.86      1.00      0.92      1663\n",
      "\n",
      "   micro avg       0.90      0.85      0.88      4808\n",
      "   macro avg       0.91      0.85      0.87      4808\n",
      "weighted avg       0.91      0.85      0.87      4808\n",
      " samples avg       0.91      0.85      0.87      4808\n",
      "\n",
      "Subset Accuracy: 0.5836\n",
      "Micro Precision: 0.9036091549295775\n",
      "Micro Recall: 0.8539933444259568\n",
      "Micro F1 Score: 0.8781009409751925\n",
      "Macro Precision: 0.9077905939311126\n",
      "Macro Recall: 0.849027130376587\n",
      "Macro F1 Score: 0.8712118760782778\n",
      "Weighted Precision: 0.9082743959912345\n",
      "Weighted Recall: 0.8539933444259568\n",
      "Weighted F1 Score: 0.8740496765617373\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Per-Class & Aggregate Report\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_))\n",
    "\n",
    "# --- 2. Subset Accuracy (exact match)\n",
    "subset_acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "\n",
    "# --- 3. Micro-Averaged Scores\n",
    "print(\"Micro Precision:\", precision_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro Recall:\", recall_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "print(\"Micro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='micro'))\n",
    "\n",
    "# --- 4. Macro-Averaged Scores\n",
    "print(\"Macro Precision:\", precision_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "print(\"Macro F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='macro'))\n",
    "\n",
    "# --- 5. Weighted-Averaged Scores\n",
    "print(\"Weighted Precision:\", precision_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted Recall:\", recall_score(y_true_bin, y_pred_bin, average='weighted'))\n",
    "print(\"Weighted F1 Score:\", f1_score(y_true_bin, y_pred_bin, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30fbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1fd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
